{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory=\"2600m\"\n",
    "launcher.conf.set(\"spark.sql.warehouse.dir\", \"hdfs://bd-hm:9000/user/hive/warehouse\")\n",
    "launcher.conf.set(\"spark.sql.catalogImplementation\",\"hive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in ratings data as RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bd-hm:8088/proxy/application_1572480568928_0001\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = yarn, app id = application_1572480568928_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1#1097#1907#2321#2018#260#938#1246#2028#150#3408#2340#919#527#914#3186#1270#2355#48#531#2692#783#661#608#745#1035#2918#1022#2687#1028#1029#2804#1836#2797#2398#2791#1961#1287#1721#1962#1545#2294#1193#588#1197#3114#595#2762#1566#594#1207#720#3105\n",
      "2\t1103#1372#1096#1917#1370#2194#1090#3809#1357#21#2728#265#2717#1610#2236#515#1873#1124#318#2490#292#1385#780#3578#2268#349#2501#3471#3468#1597#3068#1834#1293#3071#95#3735#1544#3255#368#110#380#3256#3257#589#1537#590#356#593#3035#1084#1801#2278#3030#1552#1792#2852#2067#1217#3699#2312#1225#2858#2321#3418#3147#1244#1245#1246#1784#1247#2028#2002#1253#442#1527#2006#648#920#3678#163#647#434#1259#165#2353#3451#2359#1265#902#3654#2571#1968#2916#736#3334#3095#1957#1408#1953#459#1955#457#1954#3893#2126#2396#2943#1962#2881#235#1188#1193#982#1198#498#1196#1945#1442#1687#2628#1207#1210#3107#3105#480#2427#3108#1690#1213\n",
      "3\t1641#3534#552#2735#260#2997#3552#1615#2470#1394#1378#1379#1136#1580#1304#1049#1291#590#104#593#1079#2617#2858#2871#3421#653#2006#648#1261#1259#3671#1270#2355#2081#1266#1265#3168#1968#1431#2115#1961#3619#2167#1198#1197#1196#3114#3868#733#1210#480\n",
      "4\t3527#1097#260#2951#2947#1387#2692#1036#3468#3702#3418#1240#2028#2366#1954#1198#1196#1201#1210#1214#480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = hdfs://bd-hm:9000/hadoop-user/data/ratings MapPartitionsRDD[1] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd= sc.textFile(\"hdfs://bd-hm:9000/hadoop-user/data/ratings\")\n",
    "rdd.take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,1)\n",
      "(1,1097)\n",
      "(1,1907)\n",
      "(1,2321)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ratings_pairs: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[3] at flatMapValues at <console>:26\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratings_pairs= rdd.map(line=>(line.split(\"\\t\")(0), line.split(\"\\t\")(1))).flatMapValues(l=>l.split(\"[\\\\t#]\"))\n",
    "ratings_pairs.take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn RDD into DF and save table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Table `ratings` already exists.;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Table `ratings` already exists.;",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:424)",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)",
      "  ... 37 elided",
      ""
     ]
    }
   ],
   "source": [
    "val ratings_cols=Seq(\"user\", \"movie\")\n",
    "ratings_pairs.toDF(ratings_cols:_*).write.saveAsTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|movie|\n",
      "+----+-----+\n",
      "|   1|    1|\n",
      "|   1| 1097|\n",
      "|   1| 1907|\n",
      "|   1| 2321|\n",
      "|   1| 2018|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql(\"select * from ratings\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in movies data as RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1#Toy Story (1995)#Animation|Children's|Comedy\n",
      "2#Jumanji (1995)#Adventure|Children's|Fantasy\n",
      "3#Grumpier Old Men (1995)#Comedy|Romance\n",
      "4#Waiting to Exhale (1995)#Comedy|Drama\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rddMovies: org.apache.spark.rdd.RDD[String] = hdfs://bd-hm:9000/hadoop-user/data/movies MapPartitionsRDD[9] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddMovies= sc.textFile(\"hdfs://bd-hm:9000/hadoop-user/data/movies\")\n",
    "rddMovies.take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,Toy Story (1995))\n",
      "(2,Jumanji (1995))\n",
      "(3,Grumpier Old Men (1995))\n",
      "(4,Waiting to Exhale (1995))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movies_pairs: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[10] at map at <console>:26\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies_pairs= rddMovies.map(line=>(line.split(\"#\")(0), line.split(\"#\")(1)))\n",
    "movies_pairs.take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn RDD into DF and save table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Table `movies` already exists.;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Table `movies` already exists.;",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:424)",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)",
      "  ... 37 elided",
      ""
     ]
    }
   ],
   "source": [
    "val movies_cols=Seq(\"movie\", \"title\")\n",
    "movies_pairs.toDF(movies_cols:_*).write.saveAsTable(\"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|movie|               title|\n",
      "+-----+--------------------+\n",
      "| 2020|Dangerous Liaison...|\n",
      "| 2021|         Dune (1984)|\n",
      "| 2022|Last Temptation o...|\n",
      "| 2023|Godfather: Part I...|\n",
      "| 2024| Rapture, The (1991)|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql(\"select * from movies\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write query to extract final metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------------+--------------------+\n",
      "|user1|user2|count_common_movies|         titles_list|\n",
      "+-----+-----+-------------------+--------------------+\n",
      "| 1680| 1941|               1169|[Airport '77 (197...|\n",
      "| 1181| 1680|               1013|[Titan A.E. (2000...|\n",
      "| 1680| 2063|                977|[Vampire in Brook...|\n",
      "| 1181| 1941|                938|[Gremlins (1984),...|\n",
      "| 1941| 2909|                936|[Airport '77 (197...|\n",
      "| 1680|  424|                925|[Gremlins (1984),...|\n",
      "| 1680| 1980|                923|[Gremlins (1984),...|\n",
      "| 1941| 2063|                920|[Primal Fear (199...|\n",
      "| 1680|  889|                916|[Before and After...|\n",
      "| 1015| 1680|                909|[Primal Fear (199...|\n",
      "+-----+-----+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql(\"select r1.user user1, r2.user user2, count (distinct r1.movie) count_common_movies, collect_set(r3.title) titles_list from ratings r1, ratings r2, movies r3 where r1.movie=r2.movie and r1.user<r2.user and r1.movie=r3.movie group by r1.user, r2.user having count (distinct r1.movie) > 50 order by count_common_movies desc\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
