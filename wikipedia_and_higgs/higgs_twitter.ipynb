{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory=\"1600m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bd-hm:8088/proxy/application_1571094080322_0008\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = yarn, app id = application_1571094080322_0008)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Import\n",
    "import org.apache.spark.graphx._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "1 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = hdfs://bd-hm:9000/hadoop-user/data/higgs-social_network.edgelist MapPartitionsRDD[1] at textFile at <console>:29\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Extract data\n",
    "val rdd= sc.textFile(\"hdfs://bd-hm:9000/hadoop-user/data/higgs-social_network.edgelist\")\n",
    "rdd.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,2)\n",
      "(1,3)\n",
      "(1,4)\n",
      "(1,5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "edges: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[3] at map at <console>:30\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Create pair rdd of form (userid_1, userid2)\n",
    "val edges=rdd.map(line=>line.split(\" \")).map(x=>(x(0).toLong,x(1).toLong))\n",
    "edges.take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[Null,Int] = org.apache.spark.graphx.impl.GraphImpl@4a33e36e\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Create graph\n",
    "val graph = Graph.fromEdgeTuples(edges,null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nv: Long = 456626\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Find number of vertices\n",
    "val nv=graph.numVertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185012,15)\n",
      "(129434,39)\n",
      "(182316,15)\n",
      "(194402,2)\n",
      "(199516,8)\n",
      "(332918,1)\n",
      "(170792,3)\n",
      "(307248,2)\n",
      "(32676,53)\n",
      "(38926,28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inDeg: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[18] at RDD at VertexRDD.scala:57\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Extract in-degrees for each node\n",
    "val inDeg=graph.inDegrees\n",
    "inDeg.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273,37)\n",
      "(1863,1)\n",
      "(1148,1)\n",
      "(1119,1)\n",
      "(736,4)\n",
      "(3638,1)\n",
      "(1245,1)\n",
      "(312,19)\n",
      "(62,601)\n",
      "(1184,1)\n",
      "(1625,1)\n",
      "(1566,1)\n",
      "(734,6)\n",
      "(3454,1)\n",
      "(2329,1)\n",
      "(279,32)\n",
      "(1803,1)\n",
      "(2033,1)\n",
      "(479,5)\n",
      "(2114,1)\n",
      "(3939,1)\n",
      "(2303,1)\n",
      "(2277,1)\n",
      "(989,3)\n",
      "(642,9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "onlyDeg: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[19] at map at <console>:30\n",
       "countDeg: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:32\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Extract only the in-degrees\n",
    "val onlyDeg=inDeg.map(x=>x.toString.split(\",\")(1).replaceAll(\"\\\\)\", \"\"))\n",
    "//Count # of occurences of each in-degree value\n",
    "val countDeg=onlyDeg.map(x=>(x,1)).reduceByKey((v1,v2)=>v1+v2)\n",
    "countDeg.take(25).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273,8.102912E-5)\n",
      "(1863,2.189976E-6)\n",
      "(1148,2.189976E-6)\n",
      "(1119,2.189976E-6)\n",
      "(736,8.759904E-6)\n",
      "(3638,2.189976E-6)\n",
      "(1245,2.189976E-6)\n",
      "(312,4.1609546E-5)\n",
      "(62,0.0013161756)\n",
      "(1184,2.189976E-6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pk: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[22] at map at <console>:30\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Output file of form (k, p(k)) where k is in-degree value and p(k) is probability\n",
    "val pk = countDeg.map(x=>(x.toString.split(\",\")(0).replaceAll(\"\\\\(\", \"\"), x.toString.split(\",\")(1).replaceAll(\"\\\\)\", \"\").toFloat/456626))\n",
    "\n",
    "pk.take(10).foreach(println)\n",
    "//Save p(k) file\n",
    "pk.saveAsTextFile(\"file:///home/administrator/Desktop/edgelist_output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
