{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory='6000m'\n",
    "launcher.packages=[\"com.github.master:spark-stemming_2.10:0.2.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Exploration\n",
    "Read-in Yelp review data, take a look at the data layout, and view a few sample rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bd-hm:8088/proxy/application_1573932525645_0001\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = yarn, app id = application_1573932525645_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n",
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|      date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|0W4lkclzZThpx3V65...|   0|2016-05-28|    0|v0i_UHJMo_hPBq9bx...|    5|Love the staff, l...|     0|bv2nCi5Qv5vroFiqK...|\n",
      "|AEx2SYEUJmTxVVB18...|   0|2016-05-28|    0|vkVSCC7xljjrAI4UG...|    5|Super simple plac...|     0|bv2nCi5Qv5vroFiqK...|\n",
      "|VR6GpWIda3SfvPC-l...|   0|2016-05-28|    0|n6QzIUObkYshz4dz2...|    5|Small unassuming ...|     0|bv2nCi5Qv5vroFiqK...|\n",
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "[0W4lkclzZThpx3V65bVgig,0,2016-05-28,0,v0i_UHJMo_hPBq9bxWvW4w,5,Love the staff, love the meat, love the place. Prepare for a long line around lunch or dinner hours. \n",
      "\n",
      "They ask you how you want you meat, lean or something maybe, I can't remember. Just say you don't want it too fatty. \n",
      "\n",
      "Get a half sour pickle and a hot pepper. Hand cut french fries too.,0,bv2nCi5Qv5vroFiqKGopiw]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rev: org.apache.spark.sql.DataFrame = [business_id: string, cool: bigint ... 7 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Read the CSV file and load it into a dataframe. Note that the \"inferschema\" parameter is set to true\n",
    "val rev=spark.read.option(\"header\",\"true\").option(\"inferschema\", \"true\").json(\"/hadoop-user/data/review.json\")\n",
    "rev.cache()\n",
    "rev.printSchema()\n",
    "rev.show(3)\n",
    "rev.count\n",
    "rev.take(1).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find distribution of \"stars\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n",
      "|stars|count(review_id)|\n",
      "+-----+----------------+\n",
      "|    5|         2253348|\n",
      "|    1|          731363|\n",
      "|    3|          615481|\n",
      "|    2|          438161|\n",
      "|    4|         1223316|\n",
      "+-----+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rev2: org.apache.spark.sql.DataFrame = [business_id: string, cool: bigint ... 7 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rev2=rev.toDF()\n",
    "rev2.createOrReplaceTempView(\"review\")\n",
    "spark.sql(\"select stars,count(review_id) from review group by stars\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering\n",
    "Turn \"star\" reviews of 4 and 5 into a '1' and \"star\" reviews of 1, 2, and 3 into '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+------+\n",
      "|         business_id|cool|      date|funny|           review_id|stars|                text|useful|             user_id|rating|\n",
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+------+\n",
      "|0W4lkclzZThpx3V65...|   0|2016-05-28|    0|v0i_UHJMo_hPBq9bx...|    5|Love the staff, l...|     0|bv2nCi5Qv5vroFiqK...|     1|\n",
      "|AEx2SYEUJmTxVVB18...|   0|2016-05-28|    0|vkVSCC7xljjrAI4UG...|    5|Super simple plac...|     0|bv2nCi5Qv5vroFiqK...|     1|\n",
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{when, _}\n",
       "import spark.sqlContext.implicits._\n",
       "rev3: org.apache.spark.sql.DataFrame = [business_id: string, cool: bigint ... 8 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{when,_}\n",
    "import spark.sqlContext.implicits._\n",
    "val rev3=rev2.withColumn(\"rating\", expr(\"case when stars = 4 then 1 \" +\n",
    "                                       \"when stars = 5 then 1 \" +\n",
    "                                       \"else 0 end \"))\n",
    "rev3.createOrReplaceTempView(\"review\")\n",
    "rev3.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of rating before downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|rating|count(review_id)|\n",
      "+------+----------------+\n",
      "|     1|         3476664|\n",
      "|     0|         1785005|\n",
      "+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select rating,count(review_id) from review group by rating\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rating attribute is not balanced, with a rating of 1 being nearly twice as likely as a rating of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will downsample the data, and only use 10% of the downsampled data to make the dataset size manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|rating| count|\n",
      "+------+------+\n",
      "|     1|178580|\n",
      "|     0|178418|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrameStatFunctions\n",
       "frac: scala.collection.immutable.Map[Int,Double] = Map(0 -> 0.1, 1 -> 0.05134246507571626)\n",
       "rev4: org.apache.spark.sql.DataFrame = [business_id: string, cool: bigint ... 8 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrameStatFunctions\n",
    "val frac= Map(0 -> .1, 1 -> 1785005.0/34766640.0)\n",
    "val rev4 = rev3.stat.sampleBy(\"rating\", frac, 111)\n",
    "rev4.cache().groupBy(\"rating\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retain only rating and text values from the dataset, and remove any rows with missing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating_data2: org.apache.spark.sql.DataFrame = [rating: int, text: string]\n",
       "rating_data: org.apache.spark.sql.DataFrame = [text_field: string, rating: int]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev4.createOrReplaceTempView(\"rev4\")\n",
    "val rating_data2 = spark.sql(\"select rating, text from rev4\").toDF()\n",
    "val rating_data= rating_data2.filter(\"trim(text)!='' or trim(text)!=null\").select($\"text\".alias(\"text_field\"), $\"rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline to extract TFIDF vectors from data. The pipeline will remove punctuation, remove stop words, stem the words, vectorize them and turn them into TFIDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.sql.functions.udf\n",
       "import org.apache.spark.mllib.feature.Stemmer\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_7b9db5ab3be0\n",
       "removePunc: (words: Seq[String])Seq[String]\n",
       "puncRemover: org.apache.spark.ml.feature.SQLTransformer = sql_938dbbbde055\n",
       "stopWordRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_86a87e0a016e\n",
       "stemmer: org.apache.spark.mllib.feature.Stemmer = stemmer_e5c362935948\n",
       "vectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_1c8cef4c6415\n",
       "tfidf: org.apache.spark.ml.feature.IDF = idf_8c30f17438df\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.mllib.feature.Stemmer\n",
    "\n",
    "val tokenizer = new RegexTokenizer().setMinTokenLength(3).setToLowercase(true).setInputCol(\"text_field\").setOutputCol(\"text_words\")\n",
    "\n",
    "//Defining a udf to remove punctuations from a sequence of words\n",
    "def removePunc(words:Seq[String]):Seq[String]={\n",
    " return words.map(_.replaceAll(\"\\\\p{Punct}\",\" \"))\n",
    "}\n",
    "\n",
    "//val removePuncUDF=udf(removePunc(_:Seq[String]))\n",
    "spark.udf.register(\"removePuncUDF\",removePunc(_:Seq[String]) )\n",
    "\n",
    "//use the removePuncUDF to remove all punctuation \n",
    "val puncRemover = new SQLTransformer().setStatement(\"SELECT removePuncUDF(text_words) as text_field, rating from __THIS__ \")\n",
    "val stopWordRemover=new StopWordsRemover().setInputCol(\"text_field\").setOutputCol(\"filtered_text\")\n",
    "val stemmer = new Stemmer().setInputCol(\"filtered_text\").setOutputCol(\"stemmed_text\")\n",
    "val vectorizer = new CountVectorizer().setMinDF(100).setInputCol(\"stemmed_text\").setOutputCol(\"text_BOW\")\n",
    "val tfidf = new IDF().setInputCol(\"text_BOW\").setOutputCol(\"text_TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Machine Learning Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n",
    "\n",
    "Implement Logistic Regression with 3-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------------------+\n",
      "|rating|prediction|         probability|        stemmed_text|\n",
      "+------+----------+--------------------+--------------------+\n",
      "|     0|       1.0|[0.16141907685102...|[ ovr , rate,    ...|\n",
      "|     0|       0.0|[0.91705661215902...|[ 16 50, buger, r...|\n",
      "|     0|       1.0|[0.37502600527578...|[beer, asid,  whi...|\n",
      "|     0|       1.0|[0.41504192095022...|[ 33, resort, fee...|\n",
      "|     0|       0.0|[0.99983369576327...|[shop, wasn t, vi...|\n",
      "+------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "AUC for LR on test data = 0.9440929929897601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_e26d13b3450f\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_e26d13b3450f-elasticNetParam: 0.0,\n",
       "\tidf_a072585a67a4-minDocFreq: 5,\n",
       "\tlogreg_e26d13b3450f-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_e26d13b3450f-elasticNetParam: 0.0,\n",
       "\tidf_a072585a67a4-minDocFreq: 5,\n",
       "\tlogreg_e26d13b3450f-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_e26d13b3450f-elasticNetParam: 0.0,\n",
       "\tidf_a072585a67a4-minDocFreq: 5,\n",
       "\tlogreg_e26d13b3450f-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_e26d13b3450f-elasticNetParam: 0.0,\n",
       "\tidf_a072585a67a4-minDocFreq: 10,\n",
       "\tlogreg_e26d13b3450f-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_e26d13b3450f-elasticNetParam: 0.0,\n",
       "\tidf_a072585a67a4-minDocFreq: 10,\n",
       "\tlogreg_e26d13b3450f-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_e..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "val lr = new LogisticRegression().setLabelCol(\"rating\").setFeaturesCol(\"text_TFIDF\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, Array(0.01, 0.5, 2.0))\n",
    "             .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "             .addGrid(tfidf.minDocFreq, Array(5,10))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "val cv = new CrossValidator().setEstimator(lr).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer, vectorizer, tfidf,cv))\n",
    "\n",
    "val Array(training,testing)=rating_data.randomSplit(Array(0.8,0.2),111)\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "predictions.select(\"rating\", \"prediction\", \"probability\", \"stemmed_text\").show(5)\n",
    "\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"AUC for LR on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest\n",
    "\n",
    "Implement Random Forest with 3-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------------------+\n",
      "|rating|prediction|         probability|        stemmed_text|\n",
      "+------+----------+--------------------+--------------------+\n",
      "|     0|       1.0|[0.47019936708999...|[ ovr , rate,    ...|\n",
      "|     0|       0.0|[0.53130756207492...|[ 16 50, buger, r...|\n",
      "|     0|       0.0|[0.50456762529794...|[beer, asid,  whi...|\n",
      "|     0|       1.0|[0.47151702017081...|[ 33, resort, fee...|\n",
      "|     0|       0.0|[0.54541499699992...|[shop, wasn t, vi...|\n",
      "+------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "AUC for RF on test data = 0.7886449721370802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_5854619fdaba\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tidf_a072585a67a4-minDocFreq: 5,\n",
       "\trfc_5854619fdaba-numTrees: 5\n",
       "}, {\n",
       "\tidf_a072585a67a4-minDocFreq: 10,\n",
       "\trfc_5854619fdaba-numTrees: 5\n",
       "}, {\n",
       "\tidf_a072585a67a4-minDocFreq: 5,\n",
       "\trfc_5854619fdaba-numTrees: 10\n",
       "}, {\n",
       "\tidf_a072585a67a4-minDocFreq: 10,\n",
       "\trfc_5854619fdaba-numTrees: 10\n",
       "}, {\n",
       "\tidf_a072585a67a4-minDocFreq: 5,\n",
       "\trfc_5854619fdaba-numTrees: 15\n",
       "}, {\n",
       "\tidf_a072585a67a4-minDocFreq: 10,\n",
       "\trfc_5854619fdaba-numTrees: 15\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_b97143e4f40f\n",
       "cv: org.apache.spark.ml.tuning.CrossValidator..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"rating\").setFeaturesCol(\"text_TFIDF\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, Array(5,10,15))\n",
    "             .addGrid(tfidf.minDocFreq, Array(5,10))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "val cv = new CrossValidator().setEstimator(rf).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer, vectorizer, tfidf, cv))\n",
    "\n",
    "val Array(training,testing)=rating_data.randomSplit(Array(0.8,0.2),111)\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "predictions.select(\"rating\", \"prediction\", \"probability\", \"stemmed_text\").show(5)\n",
    "\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"AUC for RF on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GB Classification\n",
    "\n",
    "Implement GB Classification with 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------------------+\n",
      "|rating|prediction|         probability|        stemmed_text|\n",
      "+------+----------+--------------------+--------------------+\n",
      "|     0|       0.0|[0.59825391532020...|[ ovr , rate,    ...|\n",
      "|     0|       0.0|[0.62839190655241...|[ 16 50, buger, r...|\n",
      "|     0|       1.0|[0.22131837077660...|[beer, asid,  whi...|\n",
      "|     0|       0.0|[0.56818077137962...|[ 33, resort, fee...|\n",
      "|     0|       0.0|[0.80144694198160...|[shop, wasn t, vi...|\n",
      "+------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "AUC for GBT on test data = 0.8300411449788886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_6209bebe4ae4\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tgbtc_6209bebe4ae4-maxDepth: 2,\n",
       "\tgbtc_6209bebe4ae4-maxIter: 5,\n",
       "\tidf_5bc1d685964d-minDocFreq: 5\n",
       "}, {\n",
       "\tgbtc_6209bebe4ae4-maxDepth: 5,\n",
       "\tgbtc_6209bebe4ae4-maxIter: 5,\n",
       "\tidf_5bc1d685964d-minDocFreq: 5\n",
       "}, {\n",
       "\tgbtc_6209bebe4ae4-maxDepth: 2,\n",
       "\tgbtc_6209bebe4ae4-maxIter: 10,\n",
       "\tidf_5bc1d685964d-minDocFreq: 5\n",
       "}, {\n",
       "\tgbtc_6209bebe4ae4-maxDepth: 5,\n",
       "\tgbtc_6209bebe4ae4-maxIter: 10,\n",
       "\tidf_5bc1d685964d-minDocFreq: 5\n",
       "}, {\n",
       "\tgbtc_6209bebe4ae4-maxDepth: 2,\n",
       "\tgbtc_6209bebe4ae4-maxIter: 5,\n",
       "\tidf_5bc1d685964d-minDocFreq: 10\n",
       "}, {\n",
       "\tgbtc_6209bebe4ae4-maxDepth: 5,\n",
       "\tgbtc_6209bebe4ae4-maxIter: 5,\n",
       "\tidf_5bc1d68..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "\n",
    "val gbt = new GBTClassifier().setLabelCol(\"rating\").setFeaturesCol(\"text_TFIDF\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, Array(2,5))\n",
    "             .addGrid(gbt.maxIter, Array(5, 10))\n",
    "             .addGrid(tfidf.minDocFreq, Array(5,10))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "val cv = new CrossValidator().setEstimator(gbt).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer, vectorizer, tfidf,cv))\n",
    "\n",
    "val Array(training,testing)=rating_data.randomSplit(Array(0.8,0.2),111)\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "\n",
    "predictions.select(\"rating\", \"prediction\", \"probability\", \"stemmed_text\").show(5)\n",
    "\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"AUC for GBT on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adding More Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in user data file, and observe the layout of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- average_stars: double (nullable = true)\n",
      " |-- compliment_cool: long (nullable = true)\n",
      " |-- compliment_cute: long (nullable = true)\n",
      " |-- compliment_funny: long (nullable = true)\n",
      " |-- compliment_hot: long (nullable = true)\n",
      " |-- compliment_list: long (nullable = true)\n",
      " |-- compliment_more: long (nullable = true)\n",
      " |-- compliment_note: long (nullable = true)\n",
      " |-- compliment_photos: long (nullable = true)\n",
      " |-- compliment_plain: long (nullable = true)\n",
      " |-- compliment_profile: long (nullable = true)\n",
      " |-- compliment_writer: long (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- elite: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- fans: long (nullable = true)\n",
      " |-- friends: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- yelping_since: string (nullable = true)\n",
      "\n",
      "+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+-----+----+--------------------+-----+------+------------+------+--------------------+-------------+\n",
      "|average_stars|compliment_cool|compliment_cute|compliment_funny|compliment_hot|compliment_list|compliment_more|compliment_note|compliment_photos|compliment_plain|compliment_profile|compliment_writer|cool|elite|fans|             friends|funny|  name|review_count|useful|             user_id|yelping_since|\n",
      "+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+-----+----+--------------------+-----+------+------------+------+--------------------+-------------+\n",
      "|         4.67|              0|              0|               0|             0|              0|              0|              0|                0|               1|                 0|                0|   0|   []|   0|[cvVMmlU1ouS3I5fh...|    0|Johnny|           8|     0|oMy_rEb0UBEmMlu-z...|   2014-11-03|\n",
      "|          3.7|              0|              0|               0|             0|              0|              0|              0|                0|               0|                 0|                0|   0|   []|   0|[0njfJmB-7n84DlIg...|    0| Chris|          10|     0|JJ-aSuM4pCFPdkfoZ...|   2013-09-24|\n",
      "|          2.0|              0|              0|               0|             0|              0|              0|              0|                0|               0|                 0|                0|   0|   []|   0|                  []|    0| Tiffy|           1|     0|uUzsFQn_6cXDh6rPN...|   2017-03-02|\n",
      "+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+-----+----+--------------------+-----+------+------------+------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user: org.apache.spark.sql.DataFrame = [average_stars: double, compliment_cool: bigint ... 20 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val user=spark.read.option(\"header\",\"true\").option(\"inferschema\", \"true\").json(\"/hadoop-user/data/user.json\")\n",
    "user.cache()\n",
    "user.printSchema()\n",
    "user.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only relevant columns from rating and user datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_data: org.apache.spark.sql.DataFrame = [average_stars: double, user_id: string]\n",
       "rating_data: org.apache.spark.sql.DataFrame = [rating: int, text: string ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.createOrReplaceTempView(\"user\")\n",
    "val user_data = spark.sql(\"select average_stars, user_id from user\").toDF()\n",
    "val rating_data = spark.sql(\"select rating, text, user_id from rev4\").toDF()\n",
    "user_data.createOrReplaceTempView(\"user_data\")\n",
    "rating_data.createOrReplaceTempView(\"rating_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join datasets together on user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-------------+--------------------+\n",
      "|rating|                text|             user_id|average_stars|             user_id|\n",
      "+------+--------------------+--------------------+-------------+--------------------+\n",
      "|     0|We got our buffet...|-3i9bhfvrM3F1wsC9...|         4.06|-3i9bhfvrM3F1wsC9...|\n",
      "|     1|Un endroit sympat...|-7JSlmBJKUQwREG_y...|          4.5|-7JSlmBJKUQwREG_y...|\n",
      "|     1|I was unsure goin...|-7JSlmBJKUQwREG_y...|          4.5|-7JSlmBJKUQwREG_y...|\n",
      "+------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "merged_data: org.apache.spark.sql.DataFrame = [rating: int, text: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val merged_data = spark.sql(\"select * from rating_data left join user_data on rating_data.user_id=user_data.user_id\")\n",
    "merged_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate correlation between rating and user average_stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Double = 0.4569367111513306\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.toDF().stat.corr(\"rating\", \"average_stars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like average_stars and rating are moderately correlated, and average_stars could be a useful predictor in our machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only relevant columns from merged dataset, and trim text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[We got our buffet lunch comped from our few days of getting drilled at the tables.  The buffet price was reasonable anyway, around $13.\n",
      "\n",
      "It was rather small as far as Vegas buffets go, but they have all the basics: fresh carved prime rib, a few mexican and chinese options, fresh salads, plus lots of the standard comfort foods.  Nothing I tried was gross, but nothing fabulous either.  Decent, cheap buffet food.  Solid three stars.\n",
      "\n",
      "Only bad thing, we had to stand at the desk and wait about 10 minutes before getting seated... and there were numerous empty tables in all sections.  The desk people were visibly bored and disinterested with their job, life, everything and everybody.,4.06,0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rating_data: org.apache.spark.sql.DataFrame = [text_field: string, average_stars: double ... 1 more field]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rating_data= merged_data.filter(\"trim(text)!='' or trim(text)!=null\").select($\"text\".alias(\"text_field\"), $\"average_stars\", $\"rating\")\n",
    "rating_data.take(1).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create TFIDF pipeline with incorporated average_stars column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_2b61ddd8a5d5\n",
       "removePunc: (words: Seq[String])Seq[String]\n",
       "puncRemover: org.apache.spark.ml.feature.SQLTransformer = sql_013b54f232d6\n",
       "stopWordRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_4168e9929dbe\n",
       "stemmer: org.apache.spark.mllib.feature.Stemmer = stemmer_3f7371ec2a9f\n",
       "vectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_57365845bd86\n",
       "tfidf: org.apache.spark.ml.feature.IDF = idf_21c89933cb6f\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new RegexTokenizer().setMinTokenLength(3).setToLowercase(true).setInputCol(\"text_field\").setOutputCol(\"text_words\")\n",
    "\n",
    "//Defining a udf to remove punctuations from a sequence of words\n",
    "def removePunc(words:Seq[String]):Seq[String]={\n",
    " return words.map(_.replaceAll(\"\\\\p{Punct}\",\" \"))\n",
    "}\n",
    "\n",
    "//val removePuncUDF=udf(removePunc(_:Seq[String]))\n",
    "spark.udf.register(\"removePuncUDF\",removePunc(_:Seq[String]) )\n",
    "\n",
    "//use the removePuncUDF to remove all punctuation\n",
    "val puncRemover = new SQLTransformer().setStatement(\"SELECT removePuncUDF(text_words) as text_field, average_stars, rating from __THIS__ \")\n",
    "val stopWordRemover=new StopWordsRemover().setInputCol(\"text_field\").setOutputCol(\"filtered_text\")\n",
    "val stemmer = new Stemmer().setInputCol(\"filtered_text\").setOutputCol(\"stemmed_text\")\n",
    "val vectorizer = new CountVectorizer().setMinDF(100).setInputCol(\"stemmed_text\").setOutputCol(\"text_BOW\")\n",
    "val tfidf = new IDF().setInputCol(\"text_BOW\").setOutputCol(\"text_TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn average_stars into a vector, standardize the values for average_stars and merge together TFIDF data with average_stars data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorizer_numeric: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_eff6df66b748\n",
       "standardizer: org.apache.spark.ml.feature.StandardScaler = stdScal_611869f96a6b\n",
       "vectorizer_assemb: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_583ab2a97d04\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectorizer_numeric=new VectorAssembler().setInputCols(Array(\"average_stars\")).setOutputCol(\"numeric_features\")\n",
    "val standardizer=new StandardScaler().setWithMean(true).setInputCol(\"numeric_features\").setOutputCol(\"numeric_features_vector\")\n",
    "val vectorizer_assemb=new VectorAssembler().setInputCols(Array(\"numeric_features_vector\",\"text_TFIDF\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n",
    "\n",
    "Implement Logistic Regression with 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------+--------------------+--------------------+\n",
      "|rating|average_stars|prediction|         probability|        stemmed_text|\n",
      "+------+-------------+----------+--------------------+--------------------+\n",
      "|     0|          1.0|       0.0|[0.99873762830949...|[     , must, rea...|\n",
      "|     0|         4.14|       1.0|[0.11885833053694...|[star, rating , i...|\n",
      "|     1|         3.46|       0.0|[0.75733271509246...|[2pm 6pm, happi, ...|\n",
      "|     0|         1.86|       0.0|[0.85852915361655...|[half, hour, hole...|\n",
      "|     1|         3.25|       1.0|[0.00607100661132...|[coupl, week, ago...|\n",
      "+------+-------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "AUC for LR on test data = 0.9510706639881208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_d2ecbf0bcf75\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_d2ecbf0bcf75-elasticNetParam: 0.0,\n",
       "\tidf_21c89933cb6f-minDocFreq: 5,\n",
       "\tlogreg_d2ecbf0bcf75-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_d2ecbf0bcf75-elasticNetParam: 0.0,\n",
       "\tidf_21c89933cb6f-minDocFreq: 5,\n",
       "\tlogreg_d2ecbf0bcf75-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_d2ecbf0bcf75-elasticNetParam: 0.0,\n",
       "\tidf_21c89933cb6f-minDocFreq: 5,\n",
       "\tlogreg_d2ecbf0bcf75-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_d2ecbf0bcf75-elasticNetParam: 0.5,\n",
       "\tidf_21c89933cb6f-minDocFreq: 5,\n",
       "\tlogreg_d2ecbf0bcf75-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_d2ecbf0bcf75-elasticNetParam: 0.5,\n",
       "\tidf_21c89933cb6f-minDocFreq: 5,\n",
       "\tlogreg_d2ecbf0bcf75-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_d2e..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "val lr = new LogisticRegression().setLabelCol(\"rating\").setFeaturesCol(\"features\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, Array(0.01, 0.5, 2.0))\n",
    "             .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "             .addGrid(tfidf.minDocFreq, Array(5,10))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "val cv = new CrossValidator().setEstimator(lr).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, puncRemover,stopWordRemover, stemmer, vectorizer, tfidf, vectorizer_numeric,standardizer, vectorizer_assemb,cv))\n",
    "\n",
    "val Array(training,testing)=rating_data.randomSplit(Array(0.8,0.2),111)\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "predictions.select(\"rating\", \"average_stars\",\"prediction\", \"probability\", \"stemmed_text\").show(5)\n",
    "\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"AUC for LR on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest\n",
    "\n",
    "Implement Random Forest with 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------+--------------------+--------------------+\n",
      "|rating|average_stars|prediction|         probability|        stemmed_text|\n",
      "+------+-------------+----------+--------------------+--------------------+\n",
      "|     0|          1.0|       0.0|[0.52121782935173...|[     , must, rea...|\n",
      "|     0|         4.14|       1.0|[0.48076607185631...|[star, rating , i...|\n",
      "|     1|         3.46|       0.0|[0.51212080904525...|[2pm 6pm, happi, ...|\n",
      "|     0|         1.86|       0.0|[0.50059466255467...|[half, hour, hole...|\n",
      "|     1|         3.25|       1.0|[0.48686552871443...|[coupl, week, ago...|\n",
      "+------+-------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "AUC for RF on test data = 0.8463778067077127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_d08e1cefa986\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tidf_21c89933cb6f-minDocFreq: 5,\n",
       "\trfc_d08e1cefa986-numTrees: 5\n",
       "}, {\n",
       "\tidf_21c89933cb6f-minDocFreq: 5,\n",
       "\trfc_d08e1cefa986-numTrees: 10\n",
       "}, {\n",
       "\tidf_21c89933cb6f-minDocFreq: 5,\n",
       "\trfc_d08e1cefa986-numTrees: 15\n",
       "}, {\n",
       "\tidf_21c89933cb6f-minDocFreq: 10,\n",
       "\trfc_d08e1cefa986-numTrees: 5\n",
       "}, {\n",
       "\tidf_21c89933cb6f-minDocFreq: 10,\n",
       "\trfc_d08e1cefa986-numTrees: 10\n",
       "}, {\n",
       "\tidf_21c89933cb6f-minDocFreq: 10,\n",
       "\trfc_d08e1cefa986-numTrees: 15\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_a72356c55fec\n",
       "cv: org.apache.spark.ml.tuning.CrossValidator..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"rating\").setFeaturesCol(\"features\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, Array(5,10,15))\n",
    "             .addGrid(tfidf.minDocFreq, Array(5,10))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "val cv = new CrossValidator().setEstimator(rf).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, puncRemover,stopWordRemover, stemmer, vectorizer, tfidf, vectorizer_numeric,standardizer, vectorizer_assemb,cv))\n",
    "\n",
    "val Array(training,testing)=rating_data.randomSplit(Array(0.8,0.2),111)\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "predictions.select(\"rating\", \"average_stars\",\"prediction\", \"probability\", \"stemmed_text\").show(5)\n",
    "\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"AUC for RF on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GB Classification\n",
    "\n",
    "Implement GB Classification with 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------+--------------------+--------------------+\n",
      "|rating|average_stars|prediction|         probability|        stemmed_text|\n",
      "+------+-------------+----------+--------------------+--------------------+\n",
      "|     0|          1.0|       0.0|[0.92146300349403...|[     , must, rea...|\n",
      "|     0|         4.14|       1.0|[0.40821797604501...|[star, rating , i...|\n",
      "|     1|         3.46|       0.0|[0.66313501555915...|[2pm 6pm, happi, ...|\n",
      "|     0|         1.86|       0.0|[0.84490323437291...|[half, hour, hole...|\n",
      "|     1|         3.25|       1.0|[0.20281641208289...|[coupl, week, ago...|\n",
      "+------+-------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "AUC for GBT on test data = 0.8592435347342001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_c56c749ecfc0\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tgbtc_c56c749ecfc0-maxDepth: 2,\n",
       "\tgbtc_c56c749ecfc0-maxIter: 5,\n",
       "\tidf_21c89933cb6f-minDocFreq: 5\n",
       "}, {\n",
       "\tgbtc_c56c749ecfc0-maxDepth: 2,\n",
       "\tgbtc_c56c749ecfc0-maxIter: 10,\n",
       "\tidf_21c89933cb6f-minDocFreq: 5\n",
       "}, {\n",
       "\tgbtc_c56c749ecfc0-maxDepth: 2,\n",
       "\tgbtc_c56c749ecfc0-maxIter: 5,\n",
       "\tidf_21c89933cb6f-minDocFreq: 10\n",
       "}, {\n",
       "\tgbtc_c56c749ecfc0-maxDepth: 2,\n",
       "\tgbtc_c56c749ecfc0-maxIter: 10,\n",
       "\tidf_21c89933cb6f-minDocFreq: 10\n",
       "}, {\n",
       "\tgbtc_c56c749ecfc0-maxDepth: 5,\n",
       "\tgbtc_c56c749ecfc0-maxIter: 5,\n",
       "\tidf_21c89933cb6f-minDocFreq: 5\n",
       "}, {\n",
       "\tgbtc_c56c749ecfc0-maxDepth: 5,\n",
       "\tgbtc_c56c749ecfc0-maxIter: 10,\n",
       "\tidf_21c89..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "\n",
    "val gbt = new GBTClassifier().setLabelCol(\"rating\").setFeaturesCol(\"features\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, Array(2,5))\n",
    "             .addGrid(gbt.maxIter, Array(5, 10))\n",
    "             .addGrid(tfidf.minDocFreq, Array(5,10))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "val cv = new CrossValidator().setEstimator(gbt).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, puncRemover,stopWordRemover, stemmer, vectorizer, tfidf, vectorizer_numeric,standardizer, vectorizer_assemb,cv))\n",
    "\n",
    "val Array(training,testing)=rating_data.randomSplit(Array(0.8,0.2),111)\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "predictions.select(\"rating\", \"average_stars\",\"prediction\", \"probability\", \"stemmed_text\").show(5)\n",
    "\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"AUC for GBT on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all three models, it looks like adding \"average_star\" helped to improve the AUC. This happened most notably for the AUC of the random forest model which improved quite a few percentage points. Therefore, it would appear that the tendencies of certain users impacts how they grade individual restaurants. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
